<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <link rel="shortcut icon" href="myIcon.ico">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

    <meta name="keywords" content="Bohan Li">
    <meta name="description" content="Bohan Li&#39; home page">
    <meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Bohan Li&#39; Homepage</title>
    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-159069803-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics -->
    <!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>


<body>
    <div id="layout-content" style="margin-top:25px">
        <table>
            <tbody>
                <tr>
                    <td width="650">
                        <div id="toptitle">
                            <h1>Bohan Li -
                                <font face="Arial"> 李博涵 </font>
                            </h1>
                        </div>

                        <!-- <h3>Mphil Candidate</h3> -->
                        <p>
                            Email: bohan.li77_at_gmail.com<br>
                            <h4><a href="https://github.com/Arlo0o">[GitHub]</a> <a href="https://scholar.google.com/citations?user=V-YdQiAAAAAJ&hl=zh-CN">[Google Scholar]</a> </h4>
                        </p>
                        </p>
                    </td>
                    <td>
                        <img src="images/libohan.jpg" style="width:50%;max-width:50%" alt="profile photo" class="hoverZoomLink" ><br>
                    </td>
                </tr>
                <tr>
                </tr>
            </tbody>
        </table>

        <h2>Biography </h2>
        <p>
          I'm a Ph.D. student at Shanghai Jiao Tong University (SJTU) and Eastern Institute of Technology(EIT), Ningbo, advised by <a href = "https://scholar.google.com/citations?user=byaSC-kAAAAJ&hl=zh-CN">Prof. Xin Jin</a>, <a href = "https://scholar.google.com/citations?hl=zh-CN&user=_cUfvYQAAAAJ">Prof. Wenjun Zeng</a>, <a href = "https://scholar.google.com/citations?hl=zh-CN&user=syoPhv8AAAAJ">Prof. Chao Ma</a>, and <a href = "https://scholar.google.com/citations?hl=zh-CN&user=yDEavdMAAAAJ">Prof. Xiaokang Yang</a>. I'm a Visiting Scholar of BME and ECE at the National University of Singapore (NUS) and work with <a href = "https://scholar.google.com/citations?user=s_kbB4oAAAAJ&hl=en">Prof. Yueming Jin</a>. I also work with <a href = "https://scholar.google.com/citations?user=s_kbB4oAAAAJ&hl=en">Prof. Hao Zhao</a> at Tsinghua University. I did my Master's degree at South China University of Technology (SCUT) and Bachelor's degree at Northeastern University (NEU). I have also spent some time at <a href="https://www.baai.ac.cn/">BAAI</a>, <a href="https://www.gwm.com.cn/">Changcheng</a>, <a href="https://www.lixiang.com/">Lixiang</a>, <a href="https://en.megvii.com/megvii_research/">MEGVII</a>, <a href="https://www.idea.edu.cn/">IDEA</a>, <a href = "https://ailab.tencent.com/ailab/zh/index">Tencent AILab</a>, <a href="https://fuxi.163.com/laboratory">NetEase AILab</a>, <a href="https://www.phigent.ai/en/">PhiGent</a>, <a href="https://www.zte.com.cn/china/">ZTE</a>.
        </p>
        <h2>Research </h2>
        <p>
          I have a broad research interest in 3D computer vison and world modeling, including autonomous vehicles and robotics, 3D scene comprehension and generation, 3D structed information processing, representation disentanglement, and AI for Science.
        </p>


        <h2>News </h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr> -->
          <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
            <div style="overflow-y: scroll; height: 205px;">
              <style>
                div::-webkit-scrollbar {
                  width: 3px; /* Scrollbar width */
                }

                div::-webkit-scrollbar-track {
                  background: #f1f1f1; /* Track color */
                }

                div::-webkit-scrollbar-thumb {
                  background: #888; /* Thumb color */
                }

                div::-webkit-scrollbar-thumb:hover {
                  background: #555; /* Thumb color on hover */
                }
              </style>
            <p>
              <li> We have <strong>three</strong> papers accepted to <strong>ICLR 2026</strong>. </li>
              <li> We have <strong>one</strong> paper accepted to <strong>IEEE TPAMI</strong> (IF=20.4), good news on the last day of 2025. </li>
              <li> We have <strong>two</strong> papers accepted to <strong>IEEE TPAMI</strong> (IF=20.4). </li>
              <li> We have a paper accepted to <strong>NeurIPS 2025</strong>. </li>
              <li> We have a paper accepted to <strong>CoRL 2025 (Oral)</strong>. </li>
              <li> We have <strong>two</strong> papers accepted to <strong>ICCV 2025</strong>. </li>
              <li> We have a paper accepted to <strong>CVPR 2025</strong>. </li>
              <li> We have a paper accepted to <strong>NeurIPS 2024</strong>. </li>
              <li> We have <strong>two</strong> papers accepted to <strong>ECCV 2024</strong>. </li>
              <li> We have a paper accepted to <strong>IJCAI 2024 (Oral)</strong>. </li>
              <li> We have a paper accepted to <strong>AAAI 2024</strong>. </li>
              <li> We have a paper accepted to <strong>ICCV 2023</strong>. </li>
              <li> I worked as a Computer Vision Algorithm Engineer at <strong>Tencent AI Lab</strong>. </li>
            </p>
          </div>

        </tbody></table>


        <h2>
            <font>Selected Publications (*Equal Contribution, †Corresponding)</font>
        </h2>
        <table id="tbPublications" width="100%"   >
            <tbody>
                <tr>
                  <td colspan="2" height="2"></td>  
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\omninwm.png" width="200px" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">OmniNWM: Omniscient Driving Navigation World Models</a></b> <br>
                      <b>Bohan Li*</b>, <font color="gray">Zhuang Ma*, Dalong Du*, Baorui Peng, Zhujin Liang, Zhenqiang Liu, Chao Ma, Yueming Jin, Hao Zhao, Wenjun Zeng, Xin Jin†.</font><br>
                      <p><p>Arxiv<br> [ <a href="https://arxiv.org/abs/2510.18313">paper</a> ] [ <a href="https://arlo0o.github.io/OmniNWM/">project page</a> ] [ <a href="https://github.com/Ma-Zhuang/OmniNWM">code </a> ]   <img src="https://img.shields.io/github/stars/Ma-Zhuang/OmniNWM.svg?style=social&label=Star" alt="GitHub Stars"> 
                  </td>
              </tr>

              <tr>
                <td width="206">
                    <img src="images\hisop.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                </td>
                <td><b><a href="">Hierarchical Context Alignment with Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction</a></b> <br>
                    <b>Bohan Li</b>, <font color="gray">Jiajun Deng, Yasheng Sun, Xiaofeng Wang, Xin Jin†, Wenjun Zeng.</font><br>
                    <p><p>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI, IF=20.4)</b><br> [ <a href="https://ieeexplore.ieee.org/document/11328858">paper</a> ] [ <a href="https://arlo0o.github.io/hisop.github.io/">project page</a> ]  
                </td>
            </tr>


              
                <tr>
                  <td width="206">
                      <img src="images\occscene.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D Scene Generation</a></b> <br>
                      <b>Bohan Li</b>, <font color="gray">Xin Jin†, Jianan Wang, Yukai Shi, Yasheng Sun, Xiaofeng Wang, Zhuang Ma, Baao Xie, Chao Ma, Xiaokang Yang, Wenjun Zeng.</font><br>
                      <p><p>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI, IF=20.4)</b><br> [ <a href="https://ieeexplore.ieee.org/document/11139101">paper</a> ]
                  </td>
              </tr>
              
                <tr>
                    <td width="206">
                        <img src="images\uniscene.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">UniScene: Unified Occupancy-centric Driving Scene Generation</a></b> <br>
                        <b>Bohan Li*</b>, <font color="gray">Jiazhe Guo*, Hongsi Liu*, Yingshuang Zou*, Yikang Ding*, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, Shuchang Zhou, Li Zhang, Xiaojuan Qi, Hao Zhao, Mu Yang, Wenjun Zeng, Xin Jin†.</font><br>
                        <p><p>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR 2025)</b><br> [ <a href="https://arxiv.org/abs/2412.05435">paper</a> ] [ <a href="https://arlo0o.github.io/uniscene/">project page</a> ] [ <a href="https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation">code</a> ]  <img src="https://img.shields.io/github/stars/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation?style=social" alt="GitHub Stars">
                    </td>
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\navinerfplus.png" width="200px"  style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">NaviNeRF++: Towards Interpretable 3D Reconstruction via Unsupervised Disentangled Representation Learning</a></b> <br>
                      <font color="gray">Baao Xie, Zequn Zhang, Huanting Guo, Qiuyu Chen, Hu Zhu,</font> <b>Bohan Li</b>, <font color="gray">Wenjun Zeng, Xin Jin†.</font><br>
                      <p><p>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI, IF=20.4)</b><br> [ <a href="https://ieeexplore.ieee.org/abstract/document/11144453">paper</a> ]
                  </td>
              </tr>

              <tr>
                <td width="206">
                    <img src="images\uniscenev2.png" width="200px"  style="box-shadow: 4px 4px 8px #888">
                </td>
                <td><b><a href="">Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method</a></b> <br>     
                    <b>Bohan Li</b>, <font color="gray"> Xin Jin†, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng.</font><br> 
                    <p><p>Arxiv<br> [ <a href="https://arxiv.org/abs/2510.22973">paper</a> ] [ <a href="https://arlo0o.github.io/uniscenev2/">project page</a> ] [ <a href="https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2">code</a> ]  <img src="https://img.shields.io/github/stars/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation?style=social" alt="GitHub Stars">
                </td>
            </tr>

            <tr>
              <td width="206">
                  <img src="images\General-World-Models-Survey.png" width="200px"  style="box-shadow: 4px 4px 8px #888">
              </td>
              <td><b><a href="">Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond</a></b> <br>  
                <font color="gray"> Zheng Zhu*†, Xiaofeng Wang*, Wangbo Zhao*, Chen Min*, </font> <b>Bohan Li*</b>, <font color="gray">Nianchen Deng*, Min Dou*, Yuqi Wang*, Botian Shi, Kai Wang, Chi Zhang, Yang You, Zhaoxiang Zhang, Dawei Zhao, Liang Xiao, Jian Zhao, Jiwen Lu, Guan Huang.</font><br> 
                  <p><p>Arxiv<br> [ <a href="https://arxiv.org/abs/2405.03520">paper</a> ] [ <a href="https://github.com/GigaAI-research/General-World-Models-Survey">project page</a> ]   <img src="https://img.shields.io/github/stars/GigaAI-research/General-World-Models-Survey.svg?style=social&label=Star" alt="GitHub Stars">  
              </td>
          </tr>


            <tr>
              <td width="206">
                  <img src="images\orv.png" width="200px" style="box-shadow: 4px 4px 8px #888">
              </td>
              <td><b><a href="">ORV: 4D Occupancy-centric Robot Video Generation</a></b> <br>
                <font color="gray">Xiuyu Yang*</font>, <b>Bohan Li*</b>, <font color="gray"> Shaocong Xu,Nan Wang,Chongjie Ye,Zhaoxi Chen,Minghan Qin,Yikang Ding,Xin Jin,Hang Zhao, Hao Zhao†.</font><br>
                  <p><p>Arxiv<br> [ <a href="https://arxiv.org/abs/2506.03079">paper</a> ] [ <a href="https://orangesodahub.github.io/ORV/">project page</a> ] [ <a href="https://huggingface.co/datasets/Pixtella/Adv-nuSc">Dataset</a> ] [ <a href="https://github.com/OrangeSodahub/ORV">code</a> ] <img src="https://img.shields.io/github/stars/OrangeSodahub/ORV.svg?style=social&label=Star" alt="GitHub Stars">  
              </td>
          </tr>

          <tr>
            <td width="206">
                <img src="images\LightofNormals.png" width="200px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td><b><a href="">Light of Normals: Unified Feature Representation for Universal Photometric Stereo</a></b> <br>
              <font color="gray">Houyuan Chen, Hong Li, Chongjie Ye, Zhaoxi Chen,</font> <b>Bohan Li</b>, <font color="gray"> Shaocong Xu, Xianda Guo, Xuhui Liu, Yikai Wang, Baochang Zhang, Satoshi Ikehata, Boxin Shi, Anyi Rao, Hao Zhao†.</font><br>
                <p><p>ICLR 2026<br> [ <a href="https://arxiv.org/abs/2506.18882">paper</a> ] [ <a href="https://houyuanchen111.github.io/lino.github.io">project page</a> ]  [ <a href="https://huggingface.co/spaces/houyuanchen/lino">HuggingFace</a> ]  [ <a href="https://github.com/houyuanchen111/LINO_UniPS">code</a> ]  <img src="https://img.shields.io/github/stars/houyuanchen111/LINO_UniPS.svg?style=social&label=Star" alt="GitHub Stars"> 
            </td>
        </tr>


        
          <tr>
            <td width="206">
                <img src="images\LightX.png" width="200px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td><b><a href="">Light-X: Generative 4D Video Rendering with Camera and Illumination Control</a></b> <br>
              <font color="gray">Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye,</font> <b>Bohan Li</b>, <font color="gray">  Zhiguo Cao, Wei Li, Hao Zhao, Ziwei Liu †.</font><br>
                <p><p>ICLR 2026<br> [ <a href="https://arxiv.org/abs/2512.05115">paper</a> ] [ <a href="https://lightx-ai.github.io/">project page</a> ]  [ <a href="https://github.com/TQTQliu/Light-X">code</a> ]   <img src="https://img.shields.io/github/stars/TQTQliu/Light-X.svg?style=social&label=Star" alt="GitHub Stars"> 
            </td>
        </tr>

          <tr>
            <td width="206">
                <img src="images\stablemap.png" width="200px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td><b><a href="">Stability Under Scrutiny: Benchmarking Representation Paradigms for Online HD Mapping</a></b> <br>
              <font color="gray">Hao Shan, Ruikai Li, Han Jiang, Yizhe Fan, Ziyang Yan,</font> <b>Bohan Li</b>, <font color="gray"> Xiaoshuai Hao, Hao Zhao, Zhiyong Cui, Yilong Ren, Haiyang Yu†.</font><br>
                <p><p>ICLR 2026<br> [ <a href="https://arxiv.org/abs/2510.10660">paper</a> ] [ <a href="https://stablehdmap.github.io/">project page</a> ]  
            </td>
        </tr>


            <tr>
              <td width="206">
                  <img src="images\challenger.png" width="200px"  style="box-shadow: 4px 4px 8px #888">
              </td>
              <td><b><a href="">Challenger: Affordable Adversarial Driving Video Generation</a></b> <br>
                <font color="gray">Zhiyuan Xu*</font>, <b>Bohan Li*</b>, <font color="gray"> Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, Hao Zhao†.</font><br>
                  <p><p>Conference on Robot Learning (<b>CoRL 2025</b> SAFE-ROL Workshop <b>Oral</b>)<br> [ <a href=https://arxiv.org/abs/2505.15880>paper</a> ] [ <a href="https://pixtella.github.io/Challenger/">project page</a> ] [ <a href="https://huggingface.co/datasets/Pixtella/Adv-nuSc">Dataset</a> ] [ <a href="https://github.com/Pixtella/Challenger">code</a> ]  <img src="https://img.shields.io/github/stars/Pixtella/Challenger.svg?style=social&label=Star" alt="GitHub Stars">  
              </td>
          </tr>

                <tr>
                  <td width="206">
                      <img src="images\MDE.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation</a></b> <br>
                      <font color="gray"> Wenyao Zhang*, Hongsi Liu*, </font> <strong>Bohan Li*</strong>,<font color="gray"> Jiawei He, Zekun Qi, Yunnan Wang, Shengyang Zhao, Xinqiang Yu, Wenjun Zeng, Xin Jin†.</font> 
                      <p><p>International Conference on Computer Vision <b>(ICCV 2025)</b><br>  [ <a href="https://arxiv.org/abs/2510.09320">paper</a> ]  [ <a href="https://github.com/Zhangwenyao1/Hybrid-depth">code</a> ] 
                  </td>
                </tr>

              <tr>
                <td width="206">
                    <img src="images\streetgs.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                </td>
                <td><b><a href="">Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting</a></b> <br>
                    <font color="gray">Nan Wang, Yuantao Chen, Lixing Xiao, Weiqing Xiao, </font><b>Bohan Li</b>, <font color="gray">Zhaoxi Chen, Chongjie Ye, Shaocong Xu, Saining Zhang, Ziyang Yan, Pierre Merriaux, Lei Lei, Tianfan Xue, Hao Zhao†.</font><br>
                    <p><p>Neural Information Processing Systems <b>(NeurIPS 2025)</b><br> [ <a href="https://arxiv.org/abs/2506.05280?">paper</a> ]  [ <a href="https://bigcileng.github.io/bilateral-driving/">project page</a> ]  [ <a href="https://github.com/BigCiLeng/bilateral-driving">code</a> ] <img src="https://img.shields.io/github/stars/BigCiLeng/bilateral-driving.svg?style=social&label=Star" alt="GitHub Stars">
                </td>
            </tr>



            <tr>
              <td width="206">
                  <img src="images\OnePoseViaGen.png" width="200px"  style="box-shadow: 4px 4px 8px #888">
              </td>
              <td><b><a href="">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></b> <br>
                <font color="gray"> Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, </font><b>Bohan Li</b>, <font color="gray">Zhaoxi Chen, Sida Peng, Hao Zhao†.</font><br>
                  <p><p>Conference on Robot Learning ( <b>CoRL 2025 Oral</b>)<br> [ <a href="https://arxiv.org/abs/2509.07978">paper</a> ] [ <a href="https://gzwsama.github.io/OnePoseviaGen.github.io/">project page</a> ] [ <a href="https://huggingface.co/spaces/ZhengGeng/OnePoseviaGen">Huggingface</a> ] [ <a href="https://github.com/gzwsama/OnePoseviaGen">code</a> ]  <img src="https://img.shields.io/github/stars/gzwsama/OnePoseviaGen.svg?style=social&label=Star" alt="GitHub Stars">
              </td>
          </tr>


              
              <tr>
                <td width="206">
                    <img src="images\dist4d.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                </td>
                <td><b><a href="">DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation</a></b> <br>
                    <font color="gray"> Jiazhe Guo, Yikang Ding, Xiwu Chen, Shuo Chen, </font> <strong>Bohan Li</strong>,<font color="gray"> Yingshuang Zou, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Zhiheng Li, Hao Zhao†.</font><br>
                    <p><p>International Conference on Computer Vision <b>(ICCV 2025)</b><br> [
                        <a href="https://arxiv.org/abs/2503.15208">paper</a> ] [ <a href="https://royalmelon0505.github.io/DiST-4D/">page</a> ] [ <a href="https://github.com/royalmelon0505/dist4d">code</a> ] <img src="https://img.shields.io/github/stars/royalmelon0505/dist4d.svg?style=social&label=Star" alt="GitHub Stars">
                </td>
              </tr>

              <tr>
                <td width="206">
                    <img src="images\mudg.jpg" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                </td>
                <td><b><a href="">MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction</a></b> <br>
                    <font color="gray"> Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, </font> <strong>Bohan Li</strong>,<font color="gray">  Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang†.</font><br>
                    <p><p>British Machine Vision Association <b>(BMVC 2025)</b><br> [
                        <a href="https://arxiv.org/abs/2503.10604">paper</a> ] [ <a href="https://heiheishuang.xyz/mudg">page</a> ] [ <a href="https://github.com/heiheishuang/MuDG">code</a> ]  <img src="https://img.shields.io/github/stars/heiheishuang/MuDG.svg?style=social&label=Star" alt="GitHub Stars">
                </td>
              </tr>




                <tr>
                  <td colspan="2" height="5"></td>  
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\TAPTRv2.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">TAPTRv2: Attention-based Position Update Improves Tracking Any Point</a></b> <br>
                      <font color="gray"> Hongyang Li, Feng Li, Hao Zhang, Tianhe Ren, Shilong Liu,</font> <strong>Bohan Li</strong>,<font color="gray"> Zhaoyang Zeng, Lei Zhang†.</font><br>
                      <p><p>Neural Information Processing Systems <b>(NeurIPS 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2407.16291">paper</a> ]  
                  </td>
                </tr>

                <tr>
                  <td colspan="2" height="5"></td>  
                </tr>

                <tr>
                <tr>
                  <td width="206">
                      <img src="images\htcl.jpg" width="200px" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Hierarchical Temporal Context Learning for Camera-based Semantic Scene Completion</a></b> <br>
                      <b>Bohan Li</b>, <font color="gray">Jiajun Deng, Wenyao Zhang, Liang, Dalong Du, Xin Jin†, Wenjun Zeng. </font><br>
                      <p><p>European Conference on Computer Vision <b>(ECCV 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2407.02077">paper</a> ]  [ <a href="https://github.com/Arlo0o/HTCL">code</a> ]  <img src="https://img.shields.io/github/stars/Arlo0o/HTCL.svg?style=social&label=Star" alt="GitHub Stars">
                  </td>
                </tr>
                </tr>

                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\cldis.jpg" width="200px"  style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Closed-Loop Unsupervised Representation Disentanglement with β-VAE Distillation and Diffusion Probabilistic Feedback</a></b> <br>
                    <font color="gray">Xin Jin*†, </font><b>Bohan Li*</b>, <font color="gray">Baao Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng. </font><br>  
                      <p><p>European Conference on Computer Vision <b>(ECCV 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2402.02346">paper</a> ]   
                  </td>
                </tr>

                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                
                <tr>
                  <td width="206">
                      <img src="images\brgscene.jpg" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Bridging Stereo Geometry and BEV Representation with Reliable Mutual Interaction for Semantic Scene Completion</a></b> <br>
                      <strong>Bohan Li</strong>,<font color="gray"> Yasheng Sun, Zhujin Liang, Dalong Du, Zhuanghui Zhang, Xiaofeng Wang, Yunnan Wang, Xin Jin†, Wenjun Zeng.</font><br>
                      <p><p>International Joint Conference on Artificial Intelligence <b>(IJCAI 2024 Oral)</b><br> [
                          <a href="https://arxiv.org/abs/2303.13959">paper</a> ]  [ <a href="https://github.com/Arlo0o/StereoScene">code</a> ]   <img src="https://img.shields.io/github/stars/Arlo0o/StereoScene.svg?style=social&label=Star" alt="GitHub Stars">
                  </td>
                </tr>
                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                <tr>
                  <td width="206">
                      <img src="images\vpd.jpg" width="200px" height="115" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">One at a Time: Progressive Multi-step Volumetric Probability Learning for Reliable 3D Scene Perception</a></b> <br>
                      <b>Bohan Li</b>, <font color="gray">Yasheng Sun, Jingxin Dong, Zheng Zhu, Jinming Liu, Xin Jin†, Wenjun Zeng. </font><br>
                      <p><p>AAAI Conference on Artificial Intelligence <b>(AAAI 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2306.12681">paper</a> ]  
                  </td>
                </tr>
                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                <tr>
                  <td width="206">
                      <img src="images\navinerf.jpg" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation</a></b> <br>
                    <font color="gray">Baao Xie*, </font><b>Bohan Li*</b>, <font color="gray">Zequn Zhang, Junting Dong, Xin Jin†, Jingyu Yang, Wenjun Zeng. </font><br>
                      <p><p>International Conference on Computer Vision <b>(ICCV 2023)</b><br> [
                          <a href="https://arxiv.org/abs/2304.11342">paper</a> ]   [ <a href="https://github.com/Arlo0o/NaviNeRF">code</a> ]
                  </td>
                </tr>
                <tr>
                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                <tr>
                </tr>
                <tr>
                  <td width="206">
                    <img src="images\tai.png" width="200px" height="100" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Robust Scale-Aware Stereo Matching Network</a></b> <br>
                    <font color="gray">James Okae, </font><b>Bohan Li</b>, <font color="gray">Juan Du†, Yueming Hu. </font><br>
                      <p><p>IEEE Transactions on Artificial Intelligence <b>(IEEE TAI)</b><br> [
                          <a href="https://ieeexplore.ieee.org/document/9547779">paper</a> ]    
                  </td>
                </tr>


                <tr></tr>
                <tr></tr>
                <!-- <tr></tr> -->

            </tbody>
        </table>
        <h2>
            <font>Experiences </font>
        </h2>
    
        <table style="width:100%;border:0;border-spacing:0 10px;margin:0 auto">
          <tbody>
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/baai.png'>
              </td>
              <td style="width:75%">
                <papertitle>BAAI</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: Large-scale 4D Reconstruction and Generation, Controllable World Modeling
              </td>
            </tr>
          
            
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/changcheng.png'>
              </td>
              <td style="width:75%">
                <papertitle>Changcheng</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: Efficient World Modeling
              </td>
            </tr>


            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/lixiang.png'>
              </td>
              <td style="width:75%">
                <papertitle>Lixiang</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: Scalable Multi-modal Driving Scene Generation
              </td>
            </tr>
          

            
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/megvii.png'>
              </td>
              <td style="width:75%">
                <papertitle>MEGVII</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: UniScene: Unified Occupancy-centric Driving Scene Generation
              </td>
            </tr>
          

            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/tencent.png'>
              </td>
              <td style="width:75%">
                <papertitle>Tencent AILab</papertitle><br>
                Research Engineer<br>
                Topic: Large-scale 3D City Scene Generation and Reconstruction
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/netease.png'>
              </td>
              <td style="width:75%">
                <papertitle>NetEase AILab</papertitle><br>
                Research Intern<br>
                Topic: Robust 3D Perception and Reconstruction
              </td>
            </tr>


            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/siweituxin.png'>
              </td>
              <td style="width:75%">
                <papertitle>NavInfo</papertitle><br>
                Research Intern<br>
                Topic: Multi-modal Driving Scene Generation
              </td>
            </tr>

            
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/idea.png'>
              </td>
              <td style="width:75%">
                <papertitle>IDEA</papertitle><br>
                Research Intern<br>
                Topic: Occupancy-based Scene Generation, Long-term Consistent Perception
              </td>
            </tr>
          
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/phigent.png'>
              </td>
              <td style="width:75%">
                <papertitle>PhiGent Robotics</papertitle><br>
                Engineer & Research Intern<br>
                Topic: Semantic Scene Completion, Depth Estimation, World Models
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/zte.png'>
              </td>
              <td style="width:75%">
                <papertitle>ZTE</papertitle><br>
                Research Intern<br>
                Topic: Monocular Depth Estimation, Real-time Stereo Matching
              </td>
            </tr>
          </tbody></table>


        <h2>
            <font>Certificate </font>
        </h2>
        <ul style="list-style-type:none">
            <li>


                  <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8">
                        2026 CSIG First Doctoral Student Forum   <br> </font> 
                        [ <a href="https://m.csig.org.cn/22/202512/53124.html">(CSIG第一届博士生论坛) </a> ] [ <a href="https://m.alltuu.com/album/a0b59263d6a4a6a354c95b1362401b06/?menu=live">(Live) </a> ] ;  
                </p>
                

                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8">
                      2025 CSC National Scholarship   <br> </font>  (CSC国家公派奖学金) ; 
              </p>
             


                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8">
                        2025 PhD National Scholarship  <br> </font> (博士国家奖学金) ; 
                </p>
                

                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8">
                        2024 Ningbo Association for Science and Technology Achievement Award   <br> </font>  (宁波市科协重大科技成果奖) ;
              </p>
              

              
                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8">
                        2024 Huatai Securities Doctoral Science and Technology Scholarship  <br> </font>   (华泰证券科技奖学金) ; 
              </p>
              
   
              <!-- <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                <font size="3">
                    <meta charset="utf-8">
                        2022 Outstanding Graduate Scholarship;  <br> </font>
            </p> -->
   

           <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
            <font size="3">
                <meta charset="utf-8">
                IJCAI 2024 Oral Presentation ; <br> </font>
        </p>
              
            </li>
        </ul>


        <h2>
            <font>Academic Services </font>
        </h2>
        <ul style="list-style-type:none">
            <li>
                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8">
                        <strong> Conference Reviewer: </strong> <br> </font>
                </p>
                <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8"> CVPR 2025-, ICCV 2025-, ECCV 2024-, NeurIPS 2024-, ICLR 2026-, AAAI 2025-, IJCAI 2025- 
                    </font>
                </p>

                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8">
                      <strong> Journal Reviewer: </strong> <br> </font>
              </p>
              <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8"> IEEE T-PAMI, IEEE T-IP, IEEE T-MM, IEEE T-CSVT, IEEE T-AI, IEEE RA-L
                  </font>
              </p>

            </li>
        </ul>
 


        <p><center>
          <div id="clustrmaps-widget" style="width:10%">
          <script type="text/javascript" id="clstr_globe" src="https://clustrmaps.com/globe.js?d=IHdTXcKBl1BpqCoYqnVUkUvuP5Pu2pZapqOfHpm0RUU"></script>
          </div>        
          <br>
            &copy; Bohan Li | Last updated: September, 2025
          </center>
        </p>

</body>

</html>