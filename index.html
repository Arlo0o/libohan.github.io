<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <link rel="shortcut icon" href="myIcon.ico">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

    <meta name="keywords" content="Bohan Li">
    <meta name="description" content="Bohan Li&#39; home page">
    <meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Bohan Li&#39; Homepage</title>
    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-159069803-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics -->
    <!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>

<body>
    <div id="layout-content" style="margin-top:25px">
        <table>
            <tbody>
                <tr>
                    <td width="650">
                        <div id="toptitle">
                            <h1>Bohan Li -
                                <font face="Arial"> 李博涵 </font>
                            </h1>
                        </div>

                        <!-- <h3>Mphil Candidate</h3> -->
                        <p>
                            Email: bohan.li77_at_gmail.com<br>
                            <h4><a href="https://github.com/Arlo0o">[GitHub]</a> <a href="https://scholar.google.com/citations?user=V-YdQiAAAAAJ&hl=zh-CN">[Google Scholar]</a> </h4>
                        </p>
                        </p>
                    </td>
                    <td>
                        <img src="images/libohan.jpg" style="width:50%;max-width:50%" alt="profile photo" class="hoverZoomLink" ><br>
                    </td>
                </tr>
                <tr>
                </tr>
            </tbody>
        </table>

        <h2>Biography </h2>
        <p>
          I'm a Ph.D. student at Shanghai Jiao Tong University (SJTU) and Eastern Institute of Technology(EIT), Ningbo, advised by <a href = "https://scholar.google.com/citations?user=byaSC-kAAAAJ&hl=zh-CN">Prof. Xin Jin</a>, <a href = "https://scholar.google.com/citations?hl=zh-CN&user=_cUfvYQAAAAJ">Prof. Wenjun Zeng</a>, <a href = "https://scholar.google.com/citations?hl=zh-CN&user=syoPhv8AAAAJ">Prof. Chao Ma</a>, and <a href = "https://scholar.google.com/citations?hl=zh-CN&user=yDEavdMAAAAJ">Prof. Xiaokang Yang</a>. My research interests lie in esearch interest in 3D computer vison, especially focusing on 3D scene comprehension and multi-modal genration. Before joining SJTU, I was a computer vision algorithm engineer at <a href = "https://ailab.tencent.com/ailab/zh/index">Tencent AI Lab</a>, working on large-sclale 3D city scene generation and reconstruction. I did my Master degree at South China University of Technology (SCUT) and Bachelor degree at Northeastern University (NEU). I have also spent some time at <a href="https://www.baai.ac.cn/">BAAI</a>, <a href="https://www.zte.com.cn/china/">Lixiang</a>, <a href="https://en.megvii.com/megvii_research/">MEGVII Research</a>, <a href="https://www.idea.edu.cn/">IDEA</a>, <a href="https://fuxi.163.com/laboratory">NetEase AILab</a>, <a href="https://www.phigent.ai/en/">PhiGent Robtics</a>, <a href="https://www.lixiang.com/#li/">ZTE Corporation</a>.
        </p>
        <h2>Research </h2>
        <p>
          I have a broad research interest in 3D computer vison, including Autonomous Vehicles and Robotics, 3D Scene Comprehension and Generation, 3D Structed Information Processing, Representation Disentanglement, Explainability of AI systems.
        </p>


        <h2>News </h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr> -->
          <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
            <div style="overflow-y: scroll; height: 205px;">
              <style>
                div::-webkit-scrollbar {
                  width: 3px; /* Scrollbar width */
                }

                div::-webkit-scrollbar-track {
                  background: #f1f1f1; /* Track color */
                }

                div::-webkit-scrollbar-thumb {
                  background: #888; /* Thumb color */
                }

                div::-webkit-scrollbar-thumb:hover {
                  background: #555; /* Thumb color on hover */
                }
              </style>
            <p>
              <li> We have <strong>two</strong> papers accepted to <strong>IEEE TPAMI</strong>. </li>
              <li> We have a paper accepted to <strong>NeurIPS 2025</strong>. </li>
              <li> We have a paper accepted to <strong>CoRL 2025 (Oral)</strong>. </li>
              <li> We have <strong>two</strong> papers accepted to <strong>ICCV 2025</strong>. </li>
              <li> We have a paper accepted to <strong>CVPR 2025</strong>. </li>
              <li> We have a paper accepted to <strong>NeurIPS 2024</strong>. </li>
              <li> We have <strong>two</strong> papers accepted to <strong>ECCV 2024</strong>. </li>
              <li> We have a paper accepted to <strong>IJCAI 2024</strong>. </li>
              <li> We have a paper accepted to <strong>AAAI 2024</strong>. </li>
              <li> We have a paper accepted to <strong>ICCV 2023</strong>. </li>
              <li> I worked as a Computer Vision Algorithm Engineer at <strong>Tencent AI Lab</strong>. </li>
            </p>
          </div>

        </tbody></table>


        <h2>
            <font>Selected Publications (*Equal Contribution, †Corresponding)</font>
        </h2>
        <table id="tbPublications" width="100%"   >
            <tbody>
                <tr>
                  <td colspan="2" height="2"></td>  
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\occscene.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D Scene Generation</a></b> <br>
                      <b>Bohan Li</b>, <font color="gray">Xin Jin†, Jianan Wang, Yukai Shi, Yasheng Sun, Xiaofeng Wang, Zhuang Ma, Baao Xie, Chao Ma, Xiaokang Yang, Wenjun Zeng.</font><br>
                      <p><p>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI)</b><br> [ <a href="https://arxiv.org/abs/2412.11183">paper</a> ]
                  </td>
              </tr>
              
                <tr>
                    <td width="206">
                        <img src="images\uniscene.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">UniScene: Unified Occupancy-centric Driving Scene Generation</a></b> <br>
                        <b>Bohan Li*</b>, <font color="gray">Jiazhe Guo*, Hongsi Liu*, Yingshuang Zou*, Yikang Ding*, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, Shuchang Zhou, Li Zhang, Xiaojuan Qi, Hao Zhao, Mu Yang, Wenjun Zeng, Xin Jin†.</font><br>
                        <p><p>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR 2025)</b><br> [ <a href="https://arxiv.org/abs/2412.05435">paper</a> ] [ <a href="https://arlo0o.github.io/uniscene/">project page</a> ] [ <a href="https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation">code</a> ]
                    </td>
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\navinerfplus.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">NaviNeRF++: Towards Interpretable 3D Reconstruction via Unsupervised Disentangled Representation Learning</a></b> <br>
                      <font color="gray">Baao Xie, Zequn Zhang, Huanting Guo, Qiuyu Chen, Hu Zhu,</font> <b>Bohan Li</b>, <font color="gray">Wenjun Zeng, Xin Jin†.</font><br>
                      <p><p>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI)</b><br> [ <a href="https://ieeexplore.ieee.org/abstract/document/11144453">paper</a> ]
                  </td>
              </tr>

              <tr>
                <td width="206">
                    <img src="images\challenger.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                </td>
                <td><b><a href="">Challenger: Affordable Adversarial Driving Video Generation</a></b> <br>
                  <font color="gray">Zhiyuan Xu*</font>, <b>Bohan Li*</b>, <font color="gray"> Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, Hao Zhao†.</font><br>
                    <p><p>Conference on Robot Learning (<b>CoRL 2025</b> SAFE-ROL Workshop <b>Oral</b>)<br> [ <a href=https://arxiv.org/abs/2505.15880">paper</a> ] [ <a href="https://pixtella.github.io/Challenger/">project page</a> ] [ <a href="https://github.com/Pixtella/Challenger">code</a> ] [ <a href="https://huggingface.co/datasets/Pixtella/Adv-nuSc">Dataset</a> ]  
                </td>
            </tr>

            <tr>
              <td width="206">
                  <img src="images\streetgs.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
              </td>
              <td><b><a href="">Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting</a></b> <br>
                   <font color="gray">Nan Wang, Yuantao Chen, Lixing Xiao, Weiqing Xiao, </font><b>Bohan Li</b>, <font color="gray">Zhaoxi Chen, Chongjie Ye, Shaocong Xu, Saining Zhang, Ziyang Yan, Pierre Merriaux, Lei Lei, Tianfan Xue, Hao Zhao†.</font><br>
                  <p><p>Neural Information Processing Systems <b>(NeurIPS 2025)</b><br> [ <a href="https://arxiv.org/abs/2506.05280?">paper</a> ]  [ <a href="https://bigcileng.github.io/bilateral-driving/">project page</a> ]  [ <a href="https://github.com/BigCiLeng/bilateral-driving">code</a> ]
              </td>
          </tr>

            <tr>
              <td width="206">
                  <img src="images\OnePoseViaGen.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
              </td>
              <td><b><a href="">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></b> <br>
                <font color="gray"> Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, </font><b>Bohan Li</b>, <font color="gray">Zhaoxi Chen, Sida Peng, Hao Zhao†.</font><br>
                  <p><p>Conference on Robot Learning ( <b>CoRL 2025 Oral</b>)<br> [ <a href=https://arxiv.org/abs/2509.07978">paper</a> ] [ <a href="https://gzwsama.github.io/OnePoseviaGen.github.io/">project page</a> ] [ <a href="https://github.com/gzwsama/OnePoseviaGen">code</a> ] [ <a href="https://huggingface.co/spaces/ZhengGeng/OnePoseviaGen">Huggingface</a> ]  
              </td>
          </tr>

          
                <tr>
                  <td width="206">
                      <img src="images\MDE.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation</a></b> <br>
                      <font color="gray"> Wenyao Zhang*, Hongsi Liu*, </font> <strong>Bohan Li*</strong>,<font color="gray"> Jiawei He, Zekun Qi, Yunnan Wang, Shengyang Zhao, Xinqiang Yu, Wenjun Zeng, Xin Jin†.</font> 
                      <p><p>International Conference on Computer Vision <b>(ICCV 2025)</b><br>  [ <a href="https://iccv.thecvf.com/virtual/2025/poster/1164">paper</a> ] 
                  </td>
                </tr>

                
                <tr>
                  <td width="206">
                      <img src="images\dist4d.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation</a></b> <br>
                      <font color="gray"> Jiazhe Guo, Yikang Ding, Xiwu Chen, Shuo Chen, </font> <strong>Bohan Li</strong>,<font color="gray"> Yingshuang Zou, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Zhiheng Li, Hao Zhao†.</font><br>
                      <p><p>International Conference on Computer Vision <b>(ICCV 2025)</b><br> [
                          <a href="https://arxiv.org/abs/2407.16291">paper</a> ]  [ <a href="https://github.com/royalmelon0505/dist4d">code</a> ] [ <a href="https://royalmelon0505.github.io/DiST-4D/">page</a> ]
                  </td>
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\mudg.jpg" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction</a></b> <br>
                      <font color="gray"> Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, </font> <strong>Bohan Li</strong>,<font color="gray">  Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang†.</font><br>
                      <p><p>British Machine Vision Association <b>(BMVC 2025)</b><br> [
                          <a href="https://arxiv.org/abs/2503.10604">paper</a> ]  [ <a href="https://github.com/heiheishuang/MuDG">code</a> ] [ <a href="https://heiheishuang.xyz/mudg">page</a> ]
                  </td>
                </tr>




                <tr>
                  <td colspan="2" height="5"></td>  
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\TAPTRv2.png" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">TAPTRv2: Attention-based Position Update Improves Tracking Any Point</a></b> <br>
                      <font color="gray"> Hongyang Li, Feng Li, Hao Zhang, Tianhe Ren, Shilong Liu,</font> <strong>Bohan Li</strong>,<font color="gray"> Zhaoyang Zeng, Lei Zhang†.</font><br>
                      <p><p>Neural Information Processing Systems <b>(NeurIPS 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2407.16291">paper</a> ]  
                  </td>
                </tr>

                <tr>
                  <td colspan="2" height="5"></td>  
                </tr>

                <tr>
                <tr>
                  <td width="206">
                      <img src="images\htcl.jpg" width="200px" height="110" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Hierarchical Temporal Context Learning for Camera-based Semantic Scene Completion</a></b> <br>
                      <b>Bohan Li</b>, <font color="gray">Jiajun Deng, Wenyao Zhang, Liang, Dalong Du, Xin Jin†, Wenjun Zeng. </font><br>
                      <p><p>European Conference on Computer Vision <b>(ECCV 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2407.02077">paper</a> ]  [ <a href="https://github.com/Arlo0o/HTCL">code</a> ]
                  </td>
                </tr>
                </tr>

                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>

                <tr>
                  <td width="206">
                      <img src="images\cldis.jpg" width="200px" height="105" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Closed-Loop Unsupervised Representation Disentanglement with β-VAE Distillation and Diffusion Probabilistic Feedback</a></b> <br>
                    <font color="gray">Xin Jin*†, </font><b>Bohan Li*</b>, <font color="gray">Baao Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng. </font><br> (*Equal Contribution)
                      <p><p>European Conference on Computer Vision <b>(ECCV 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2402.02346">paper</a> ]   
                  </td>
                </tr>

                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                
                <tr>
                  <td width="206">
                      <img src="images\brgscene.jpg" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Bridging Stereo Geometry and BEV Representation with Reliable Mutual Interaction for Semantic Scene Completion</a></b> <br>
                      <strong>Bohan Li</strong>,<font color="gray"> Yasheng Sun, Zhujin Liang, Dalong Du, Zhuanghui Zhang, Xiaofeng Wang, Yunnan Wang, Xin Jin†, Wenjun Zeng.</font><br>
                      <p><p>International Joint Conference on Artificial Intelligence <b>(IJCAI 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2303.13959">paper</a> ]  [ <a href="https://github.com/Arlo0o/StereoScene">code</a> ]
                  </td>
                </tr>
                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                <tr>
                  <td width="206">
                      <img src="images\vpd.jpg" width="200px" height="115" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">One at a Time: Progressive Multi-step Volumetric Probability Learning for Reliable 3D Scene Perception</a></b> <br>
                      <b>Bohan Li</b>, <font color="gray">Yasheng Sun, Jingxin Dong, Zheng Zhu, Jinming Liu, Xin Jin†, Wenjun Zeng. </font><br>
                      <p><p>AAAI Conference on Artificial Intelligence <b>(AAAI 2024)</b><br> [
                          <a href="https://arxiv.org/abs/2306.12681">paper</a> ]  
                  </td>
                </tr>
                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                <tr>
                  <td width="206">
                      <img src="images\navinerf.jpg" width="200px" height="80" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation</a></b> <br>
                    <font color="gray">Baao Xie, </font><b>Bohan Li</b>, <font color="gray">Zequn Zhang, Junting Dong, Xin Jin†, Jingyu Yang, Wenjun Zeng. </font><br>
                      <p><p>International Conference on Computer Vision <b>(ICCV 2023)</b><br> [
                          <a href="https://arxiv.org/abs/2304.11342">paper</a> ]   [ <a href="https://github.com/Arlo0o/NaviNeRF">code</a> ]
                  </td>
                </tr>
                <tr>
                <tr>
                  <td colspan="2" height="10"></td>  
                </tr>
                <tr>
                </tr>
                <tr>
                  <td width="206">
                    <img src="images\tai.png" width="200px" height="100" style="box-shadow: 4px 4px 8px #888">
                  </td>
                  <td><b><a href="">Robust Scale-Aware Stereo Matching Network</a></b> <br>
                    <font color="gray">James Okae, </font><b>Bohan Li</b>, <font color="gray">Juan Du†, Yueming Hu. </font><br>
                      <p><p>IEEE Transactions on Artificial Intelligence <b>(TAI)</b><br> [
                          <a href="https://ieeexplore.ieee.org/document/9547779">paper</a> ]    
                  </td>
                </tr>


                <tr></tr>
                <tr></tr>
                <!-- <tr></tr> -->

            </tbody>
        </table>
        <h2>
            <font>Experiences </font>
        </h2>
    
        <table style="width:100%;border:0;border-spacing:0 10px;margin:0 auto">
          <tbody>
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/baai.png'>
              </td>
              <td style="width:75%">
                <papertitle>BAAI</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: Large-scale 4D Reconstruction and Generation
              </td>
            </tr>
          
    
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/lixiang.png'>
              </td>
              <td style="width:75%">
                <papertitle>Lixiang</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: Scalable Multi-modal Driving Scene Generation
              </td>
            </tr>
          
       
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/megvii.png'>
              </td>
              <td style="width:75%">
                <papertitle>MEGVII</papertitle><br>
                Project Leader & Research Intern<br>
                Topic: UniScene: Unified Occupancy-centric Driving Scene Generation
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/idea.png'>
              </td>
              <td style="width:75%">
                <papertitle>IDEA</papertitle><br>
                Research Intern<br>
                Topic: Occupancy-based Scene Generation, Long-term Consistent Perception
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/tencent.png'>
              </td>
              <td style="width:75%">
                <papertitle>Tencent AILab</papertitle><br>
                Full-time Research Engineer<br>
                Topic: Large-scale 3D City Scene Generation and Reconstruction
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/netease.png'>
              </td>
              <td style="width:75%">
                <papertitle>NetEase AILab</papertitle><br>
                Research Intern<br>
                Topic: Robust 3D Perception and Reconstruction
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/phigent.png'>
              </td>
              <td style="width:75%">
                <papertitle>PhiGent Robotics</papertitle><br>
                Engineer & Research Intern<br>
                Topic: Semantic Scene Completion, Depth Estimation, Stereo Matching
              </td>
            </tr>
          
 
            <tr>
              <td style="padding:5px;width:25%">
                <img style="width:60%;max-width:60%" src='images/zte.png'>
              </td>
              <td style="width:75%">
                <papertitle>ZTE</papertitle><br>
                Research Intern<br>
                Topic: Monocular Depth Estimation, Real-time Stereo Matching
              </td>
            </tr>
          </tbody></table>


        <h2>
            <font>Academic Services </font>
        </h2>
        <ul style="list-style-type:none">
            <li>
                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8">
                        <strong> Conference Reviewer: </strong> <br> </font>
                </p>
                <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8"> CVPR 2025-, ICCV 2025-, ECCV 2024-, NeurIPS 2024-, ICLR 2026-, AAAI 2025-, IJCAI 2025- 
                    </font>
                </p>

                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8">
                      <strong> Journal Reviewer: </strong> <br> </font>
              </p>
              <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                  <font size="3">
                      <meta charset="utf-8"> IEEE T-PAMI, IEEE T-IP, IEEE T-MM, IEEE T-CSVT, IEEE T-AI, IEEE RA-L
                  </font>
              </p>

            </li>
        </ul>
 


        <p><center>
          <div id="clustrmaps-widget" style="width:10%">
          <script type="text/javascript" id="clstr_globe" src="https://clustrmaps.com/globe.js?d=IHdTXcKBl1BpqCoYqnVUkUvuP5Pu2pZapqOfHpm0RUU"></script>
          </div>        
          <br>
            &copy; Bohan Li | Last updated: September, 2025
          </center>
        </p>

</body>

</html>